import streamlit as st
import torch
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever, ParentDocumentRetriever
from langchain.storage import InMemoryStore
from langchain_huggingface import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_transformers import LongContextReorder
import pickle
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_core.messages import AIMessage, HumanMessage
from langchain_community.chat_message_histories import ChatMessageHistory

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸŒ± ë†ì—… ê¸°ìˆ  ì±—ë´‡",
    page_icon="ğŸŒ±",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS ìŠ¤íƒ€ì¼ë§
st.markdown("""
<style>
    /* ì „ì²´ ì»¨í…Œì´ë„ˆì˜ ìµœëŒ€ ê°€ë¡œí­ ì œí•œ */
    .block-container {
        max-width: 950px !important;
        margin-left: auto !important;
        margin-right: auto !important;
    }
    /* ì „ì²´ í˜ì´ì§€ ë°°ìœ¨(zoom) ì¡°ì • */
    html {
        zoom: 1.12;
    }
    .main-header {
        font-size: 3rem;
        font-weight: bold;
        text-align: center;
        color: #2E8B57;
        margin-bottom: 2rem;
    }
    .chat-message {
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
        display: flex;
        flex-direction: column;
    }
    .user-message {
        background-color: #E8F5E8;
        border-left: 5px solid #2E8B57;
    }
    .bot-message {
        background-color: #F0F8FF;
        border-left: 5px solid #4682B4;
    }

    /* ğŸ¨ ë‹µë³€ ê°€ë…ì„± í–¥ìƒì„ ìœ„í•œ ìŠ¤íƒ€ì¼ ì¶”ê°€ */
    .bot-message p {
        line-height: 1.7; /* ë¬¸ì¥ ì¤„ê°„ê²© */
        margin-bottom: 1rem; /* ë‹¨ë½ ì•„ë˜ ì—¬ë°± */
    }
    .bot-message ul, .bot-message ol {
        padding-left: 1.5rem; /* ëª©ë¡ ë“¤ì—¬ì“°ê¸° */
        margin-bottom: 1rem; /* ëª©ë¡ ì „ì²´ ì•„ë˜ ì—¬ë°± */
    }
    .bot-message li {
        margin-bottom: 0.75rem; /* ëª©ë¡ í•­ëª© ê°„ ì—¬ë°± */
        line-height: 1.7; /* ëª©ë¡ í•­ëª© ë‚´ ì¤„ê°„ê²© */
    }
    /* ë§ˆì§€ë§‰ p, ul, ol ìš”ì†Œì˜ ì•„ë˜ ì—¬ë°± ì œê±°í•˜ì—¬ ì»¨í…Œì´ë„ˆì™€ ê°„ê²© ì¼ê´€ì„± ìœ ì§€ */
    .bot-message p:last-child,
    .bot-message ul:last-child,
    .bot-message ol:last-child {
        margin-bottom: 0;
    }
    /* --- ê¸°ì¡´ ìŠ¤íƒ€ì¼ --- */
    
    .stButton > button {
        background-color: #2E8B57;
        color: white;
        border-radius: 0.5rem;
        border: none;
        padding: 0.5rem 1rem;
        font-weight: bold;
    }
    .stButton > button:hover {
        background-color: #228B22;
    }
    .sidebar .sidebar-content {
        background-color: #F8F9FA;
    }
    .info-box {
        background-color: #E3F2FD;
        border-left: 5px solid #2196F3;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.markdown("## ğŸŒ± ë†ì—… ê¸°ìˆ  ì±—ë´‡")
    st.markdown("---")
    st.markdown("### ğŸ“š ì§€ì› ì‘ë¬¼")
    st.markdown("- ğŸ“ ë”¸ê¸°")
    st.markdown("- ğŸ¥” ê°ì")
    st.markdown("---")
    st.markdown("### ğŸ’¡ ì§ˆë¬¸ ì˜ˆì‹œ")
    st.markdown("- ë”¸ê¸°ë¥¼ ì‹¬ìœ¼ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ë¼?")
    st.markdown("- ê°ì ì¬ë°° ì‹œ ì£¼ì˜ì‚¬í•­ì€?")
    st.markdown("- ë”¸ê¸°ì™€ ê°ì ì¤‘ ì–´ë–¤ ê²Œ ë” ì‰¬ì›Œ?")
    st.markdown("- ë”¸ê¸° ë°˜ì´‰ì„±ì¬ë°° ë°©ë²•ì€?")
    st.markdown("---")
    st.markdown("### ğŸ”§ ì‚¬ìš©ëœ ê¸°ìˆ ")
    st.markdown("- **RAG (Retrieval-Augmented Generation)**")
    st.markdown("- **ParentDocumentRetriever**")
    st.markdown("- **Ensemble Retriever**")
    st.markdown("- **Long Context Reorder**")

# ë©”ì¸ í—¤ë”
st.markdown('<h1 class="main-header">ğŸŒ± ë†ì—… ê¸°ìˆ  ì±—ë´‡</h1>', unsafe_allow_html=True)
st.markdown("### ë”¸ê¸°ì™€ ê°ì ì¬ë°°ì— ëŒ€í•œ ëª¨ë“  ê¶ê¸ˆì¦ì„ í•´ê²°í•´ë“œë¦½ë‹ˆë‹¤ ğŸš€")

# ëª¨ë¸ ë¡œë”© (ìºì‹±)
@st.cache_resource
def load_models():
    """ëª¨ë¸ë“¤ì„ ë¡œë”©í•˜ê³  ìºì‹±í•˜ëŠ” í•¨ìˆ˜"""
    with st.spinner("ğŸ¤– AI ëª¨ë¸ì„ ë¡œë”©í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        # ì„ë² ë”© ëª¨ë¸
        embedding = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-nli")
        
        # LLM ëª¨ë¸
        quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)
        model_id = "naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B"
        tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_id, device_map="auto", quantization_config=quantization_config, trust_remote_code=True
        )
        pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=512)
        llm = HuggingFacePipeline(pipeline=pipe)
        
        return embedding, llm

@st.cache_resource
def create_retrievers(_embedding):
    """ë¦¬íŠ¸ë¦¬ë²„ë“¤ì„ ìƒì„±í•˜ê³  ìºì‹±í•˜ëŠ” í•¨ìˆ˜"""
    with st.spinner("ğŸ“š ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¡œë”©í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        def create_retriever_for_crop(crop_name_en: str):
            try:
                # FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë”©
                vectorstore = FAISS.load_local(f"{crop_name_en}_faiss_db", _embedding, allow_dangerous_deserialization=True)
                
                # ë¶€ëª¨ ë¬¸ì„œ ë¡œë”©
                with open(f"{crop_name_en}_parent_docs.pkl", "rb") as f: 
                    parent_documents = pickle.load(f)
                
                # ParentDocumentRetriever ì„¤ì •
                child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)
                pdr = ParentDocumentRetriever(vectorstore=vectorstore, docstore=InMemoryStore(), child_splitter=child_splitter)
                
                # BM25 ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
                bm25_retriever = BM25Retriever.from_documents(documents=parent_documents, k=5)
                
                # Ensemble ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
                ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, pdr], weights=[0.6, 0.4])
                return ensemble_retriever
            except Exception as e:
                st.error(f"âŒ {crop_name_en} ë°ì´í„°ë² ì´ìŠ¤ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
                st.error(f"ğŸ“ í™•ì¸í•  íŒŒì¼ë“¤:")
                st.error(f"   - {crop_name_en}_faiss_db/ (í´ë”)")
                st.error(f"   - {crop_name_en}_parent_docs.pkl")
                return None
        
        retrievers = {
            "ë”¸ê¸°": create_retriever_for_crop("strawberry"),
            "ê°ì": create_retriever_for_crop("potato"),
        }
        
        return retrievers

# ëª¨ë¸ ë¡œë”©
embedding, llm = load_models()
retrievers = create_retrievers(embedding)
reordering = LongContextReorder()

# ë‹µë³€ ìƒì„± í•¨ìˆ˜
def generate_answer(question, retrieved_docs):
    """ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    if not retrieved_docs:
        return "ì£„ì†¡í•˜ì§€ë§Œ, í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ëŠ” ì œê°€ ê°€ì§„ ë¬¸ì„œì— ì—†ìŠµë‹ˆë‹¤. 'ë”¸ê¸°'ë‚˜ 'ê°ì'ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
    
    # ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆœì„œ ì¬ë°°ì—´ ë° í¬ë§·íŒ…
    reordered_docs = reordering.transform_documents(retrieved_docs)
    context_text = "\n\n".join(doc.page_content for doc in reordered_docs)
    
    # ê°•í™”ëœ í”„ë¡¬í”„íŠ¸ (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€)
    answer_prompt = PromptTemplate.from_template(
        "ë‹¹ì‹ ì€ 'ë†ì—…ê¸°ìˆ ê¸¸ì¡ì´' ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” AI ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n"
        "ì£¼ì–´ì§„ 'ë‚´ìš©'ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ 'ì§ˆë¬¸'ì— ëŒ€í•´ ë‹µë³€í•˜ì„¸ìš”.\n"
        "ë‚´ìš©ì— ì—†ëŠ” ì •ë³´ëŠ” ì ˆëŒ€ë¡œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”. ì¶”ì¸¡í•˜ê±°ë‚˜ ê¾¸ë©°ë‚´ì§€ ë§ˆì„¸ìš”.\n"
        "ë‹µë³€ì€ ì™„ì „í•œ ë¬¸ì¥ í˜•íƒœë¡œ, ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n\n"
        "--- ë‚´ìš© ---\n{context}\n\n"
        "--- ì§ˆë¬¸ ---\n{question}\n\n"
        "--- ë‹µë³€ ---\n"
    )
    
    rag_chain = answer_prompt | llm | StrOutputParser()
    
    with st.spinner("ğŸ¤” ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        full_answer_output = rag_chain.invoke({
            "context": context_text,
            "question": question
        })
        
        # í”„ë¡¬í”„íŠ¸ ì „ì²´ ì¶œë ¥ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ íŒŒì‹± ë¡œì§
        try:
            # ëª¨ë¸ì´ í”„ë¡¬í”„íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ëŠ” ê²½ìš°
            final_answer = full_answer_output.split("--- ë‹µë³€ ---")[-1].strip()
            # ë§Œì•½ ë‹µë³€ì´ ë¹„ì–´ìˆë‹¤ë©´, ëª¨ë¸ì´ í”„ë¡¬í”„íŠ¸ë¥¼ ì¶œë ¥í•˜ì§€ ì•Šê³  ë°”ë¡œ ë‹µë³€í•œ ê²½ìš°ì´ë¯€ë¡œ ì›ë³¸ì„ ì‚¬ìš©
            if not final_answer:
                final_answer = full_answer_output.strip()
        except:
            # ì˜ˆì™¸ ë°œìƒ ì‹œ ì›ë³¸ ì¶œë ¥ ì‚¬ìš©
            final_answer = full_answer_output.strip()
    
    return final_answer

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

if "chat_history" not in st.session_state:
    st.session_state.chat_history = ChatMessageHistory()

# ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ë”¸ê¸°ë‚˜ ê°ìì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”!"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # ì±—ë´‡ ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        with st.spinner("ğŸ” ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            # ê·œì¹™ ê¸°ë°˜ ë¼ìš°íŒ… (ì›ë³¸ íŒŒì¼ê³¼ ë™ì¼)
            is_strawberry = "ë”¸ê¸°" in prompt or "ì„¤í–¥" in prompt or "ë°˜ì´‰ì„±ì¬ë°°" in prompt
            is_potato = "ê°ì" in prompt
            
            retrieved_docs = []
            search_info = ""
            
            if is_strawberry and is_potato:
                search_info = "ğŸ“ 'ë”¸ê¸°'ì™€ ğŸ¥” 'ê°ì' DBì—ì„œ ëª¨ë‘ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤ (ë¹„êµ ì§ˆë¬¸)."
                strawberry_docs = retrievers["ë”¸ê¸°"].invoke(prompt) if retrievers["ë”¸ê¸°"] else []
                potato_docs = retrievers["ê°ì"].invoke(prompt) if retrievers["ê°ì"] else []
                retrieved_docs = strawberry_docs + potato_docs
            elif is_strawberry:
                search_info = "ğŸ“ 'ë”¸ê¸°' DBì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."
                retrieved_docs = retrievers["ë”¸ê¸°"].invoke(prompt) if retrievers["ë”¸ê¸°"] else []
            elif is_potato:
                search_info = "ğŸ¥” 'ê°ì' DBì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."
                retrieved_docs = retrievers["ê°ì"].invoke(prompt) if retrievers["ê°ì"] else []
            
            if search_info:
                st.info(search_info)
            
            # ë‹µë³€ ìƒì„±
            response = generate_answer(prompt, retrieved_docs)
            st.markdown(response)
    
    # ì±—ë´‡ ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "assistant", "content": response})
    
    # ëŒ€í™” ê¸°ë¡ ì €ì¥
    st.session_state.chat_history.add_user_message(prompt)
    st.session_state.chat_history.add_ai_message(response)

# í•˜ë‹¨ ì •ë³´
st.markdown("---")
col1, col2, col3 = st.columns(3)
with col1:
    st.markdown("### ğŸ“Š ì‚¬ìš©ëœ ê¸°ìˆ ")
    st.markdown("- RAG (Retrieval-Augmented Generation)")
    st.markdown("- ParentDocumentRetriever")
    st.markdown("- Ensemble Retriever")

with col2:
    st.markdown("### ğŸŒ± ì§€ì› ì‘ë¬¼")
    st.markdown("- ğŸ“ ë”¸ê¸° ì¬ë°°ë²•")
    st.markdown("- ğŸ¥” ê°ì ì¬ë°°ë²•")
    st.markdown("- ğŸ“š ë†ì—… ê¸°ìˆ  ê°€ì´ë“œ")

with col3:
    st.markdown("### ğŸ’¡ ì‚¬ìš© íŒ")
    st.markdown("- êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”")
    st.markdown("- ë”¸ê¸°/ê°ì í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ì„¸ìš”")
    st.markdown("- ë¹„êµ ì§ˆë¬¸ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤")

# ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
    st.session_state.messages = []
    st.session_state.chat_history = ChatMessageHistory()
    st.rerun() 