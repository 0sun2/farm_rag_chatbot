import streamlit as st
import torch
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever, ParentDocumentRetriever
from langchain.storage import InMemoryStore
from langchain_huggingface import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_transformers import LongContextReorder
import pickle
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_core.messages import AIMessage, HumanMessage
from langchain_community.chat_message_histories import ChatMessageHistory
import time

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸŒ± ë†ì—… ê¸°ìˆ  ì±—ë´‡",
    page_icon="ğŸŒ±",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS ìŠ¤íƒ€ì¼ë§
st.markdown("""
<style>
    /* ì „ì²´ ì»¨í…Œì´ë„ˆì˜ ìµœëŒ€ ê°€ë¡œí­ ì œí•œ */
    .block-container {
        max-width: 950px !important;
        margin-left: auto !important;
        margin-right: auto !important;
    }
    /* ì „ì²´ í˜ì´ì§€ ë°°ìœ¨(zoom) ì¡°ì • */
    html {
        zoom: 1.12;
    }
    .main-header {
        font-size: 3rem;
        font-weight: bold;
        text-align: center;
        color: #2E8B57;
        margin-bottom: 2rem;
    }
    .chat-message {
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
        display: flex;
        flex-direction: column;
    }
    .user-message {
        background-color: #E8F5E8;
        border-left: 5px solid #2E8B57;
    }
    .bot-message {
        background-color: #F0F8FF;
        border-left: 5px solid #4682B4;
    }
    .message-content {
        font-size: 1rem;
        line-height: 1.5;
    }
    .message-time {
        font-size: 0.8rem;
        color: #666;
        margin-top: 0.5rem;
    }
    .stButton > button {
        background-color: #2E8B57;
        color: white;
        border-radius: 0.5rem;
        border: none;
        padding: 0.5rem 1rem;
        font-weight: bold;
    }
    .stButton > button:hover {
        background-color: #228B22;
    }
    .sidebar .sidebar-content {
        background-color: #F8F9FA;
    }
</style>
""", unsafe_allow_html=True)

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.markdown("## ğŸŒ± ë†ì—… ê¸°ìˆ  ì±—ë´‡")
    st.markdown("---")
    st.markdown("### ğŸ“š ì§€ì› ì‘ë¬¼")
    st.markdown("- ğŸ“ ë”¸ê¸°")
    st.markdown("- ğŸ¥” ê°ì")
    st.markdown("---")
    st.markdown("### ğŸ’¡ ì§ˆë¬¸ ì˜ˆì‹œ")
    st.markdown("- ë”¸ê¸°ë¥¼ ì‹¬ìœ¼ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ë¼?")
    st.markdown("- ê°ì ì¬ë°° ì‹œ ì£¼ì˜ì‚¬í•­ì€?")
    st.markdown("- ë”¸ê¸°ì™€ ê°ì ì¤‘ ì–´ë–¤ ê²Œ ë” ì‰¬ì›Œ?")
    st.markdown("---")
    st.markdown("### ğŸ”§ ì‚¬ìš©ëœ ê¸°ìˆ ")
    st.markdown("- **RAG (Retrieval-Augmented Generation)**")
    st.markdown("- **ParentDocumentRetriever**")
    st.markdown("- **Ensemble Retriever**")
    st.markdown("- **Long Context Reorder**")

# ë©”ì¸ í—¤ë”
st.markdown('<h1 class="main-header">ğŸŒ± ë†ì—… ê¸°ìˆ  ì±—ë´‡</h1>', unsafe_allow_html=True)
st.markdown("### ë”¸ê¸°ì™€ ê°ì ì¬ë°°ì— ëŒ€í•œ ëª¨ë“  ê¶ê¸ˆì¦ì„ í•´ê²°í•´ë“œë¦½ë‹ˆë‹¤! ğŸš€")

# ëª¨ë¸ ë¡œë”© (ìºì‹±)
@st.cache_resource
def load_models():
    """ëª¨ë¸ë“¤ì„ ë¡œë”©í•˜ê³  ìºì‹±í•˜ëŠ” í•¨ìˆ˜"""
    with st.spinner("ğŸ¤– AI ëª¨ë¸ì„ ë¡œë”©í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        # ì„ë² ë”© ëª¨ë¸
        embedding = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-nli")
        
        # LLM ëª¨ë¸
        quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)
        model_id = "naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B"
        tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_id, device_map="auto", quantization_config=quantization_config, trust_remote_code=True
        )
        pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=512)
        llm = HuggingFacePipeline(pipeline=pipe)
        
        return embedding, llm

@st.cache_resource
def create_retrievers(_embedding):
    """ë¦¬íŠ¸ë¦¬ë²„ë“¤ì„ ìƒì„±í•˜ê³  ìºì‹±í•˜ëŠ” í•¨ìˆ˜"""
    with st.spinner("ğŸ“š ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¡œë”©í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        def create_retriever_for_crop(crop_name_en: str):
            try:
                vectorstore = FAISS.load_local(f"{crop_name_en}_faiss_db", _embedding, allow_dangerous_deserialization=True)
                with open(f"{crop_name_en}_parent_docs.pkl", "rb") as f: 
                    parent_documents = pickle.load(f)
                
                child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)
                pdr = ParentDocumentRetriever(vectorstore=vectorstore, docstore=InMemoryStore(), child_splitter=child_splitter)
                
                bm25_retriever = BM25Retriever.from_documents(documents=parent_documents, k=5)
                ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, pdr], weights=[0.6, 0.4])
                return ensemble_retriever
            except Exception as e:
                st.error(f"âŒ {crop_name_en} ë°ì´í„°ë² ì´ìŠ¤ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
                return None
        
        retrievers = {
            "ë”¸ê¸°": create_retriever_for_crop("strawberry"),
            "ê°ì": create_retriever_for_crop("potato"),
        }
        
        return retrievers

# ëª¨ë¸ ë¡œë”©
embedding, llm = load_models()
retrievers = create_retrievers(embedding)
reordering = LongContextReorder()

# ë‹µë³€ ìƒì„± í•¨ìˆ˜
def generate_answer(question, retrieved_docs):
    """ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    if not retrieved_docs:
        return "ì£„ì†¡í•˜ì§€ë§Œ, í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ëŠ” ì œê°€ ê°€ì§„ ë¬¸ì„œì— ì—†ìŠµë‹ˆë‹¤. 'ë”¸ê¸°'ë‚˜ 'ê°ì'ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
    
    # ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆœì„œ ì¬ë°°ì—´ ë° í¬ë§·íŒ…
    reordered_docs = reordering.transform_documents(retrieved_docs)
    context_text = "\n\n".join(doc.page_content for doc in reordered_docs)
    
    # ë‹µë³€ ìƒì„±
    answer_prompt = PromptTemplate.from_template(
        "ì£¼ì–´ì§„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ìƒì„¸í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n\në‚´ìš©:\n{context}\n\nì§ˆë¬¸: {input}\n\në‹µë³€:"
    )
    rag_chain = answer_prompt | llm | StrOutputParser()
    
    with st.spinner("ğŸ¤” ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        answer = rag_chain.invoke({
            "context": context_text,
            "input": question
        })
    
    return answer

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

if "chat_history" not in st.session_state:
    st.session_state.chat_history = ChatMessageHistory()

# ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ë”¸ê¸°ë‚˜ ê°ìì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”!"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # ì±—ë´‡ ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        with st.spinner("ğŸ” ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            # ê·œì¹™ ê¸°ë°˜ ë¼ìš°íŒ…
            is_strawberry = "ë”¸ê¸°" in prompt or "ì„¤í–¥" in prompt
            is_potato = "ê°ì" in prompt
            
            retrieved_docs = []
            search_info = ""
            
            if is_strawberry and is_potato:
                search_info = "ğŸ“ 'ë”¸ê¸°'ì™€ ğŸ¥” 'ê°ì' DBì—ì„œ ëª¨ë‘ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤ (ë¹„êµ ì§ˆë¬¸)."
                strawberry_docs = retrievers["ë”¸ê¸°"].invoke(prompt) if retrievers["ë”¸ê¸°"] else []
                potato_docs = retrievers["ê°ì"].invoke(prompt) if retrievers["ê°ì"] else []
                retrieved_docs = strawberry_docs + potato_docs
            elif is_strawberry:
                search_info = "ğŸ“ 'ë”¸ê¸°' DBì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."
                retrieved_docs = retrievers["ë”¸ê¸°"].invoke(prompt) if retrievers["ë”¸ê¸°"] else []
            elif is_potato:
                search_info = "ğŸ¥” 'ê°ì' DBì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."
                retrieved_docs = retrievers["ê°ì"].invoke(prompt) if retrievers["ê°ì"] else []
            
            if search_info:
                st.info(search_info)
            
            # ë‹µë³€ ìƒì„±
            response = generate_answer(prompt, retrieved_docs)
            st.markdown(response)
    
    # ì±—ë´‡ ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "assistant", "content": response})
    
    # ëŒ€í™” ê¸°ë¡ ì €ì¥
    st.session_state.chat_history.add_user_message(prompt)
    st.session_state.chat_history.add_ai_message(response)

# í•˜ë‹¨ ì •ë³´
st.markdown("---")
col1, col2, col3 = st.columns(3)
with col1:
    st.markdown("### ğŸ“Š ì‚¬ìš©ëœ ê¸°ìˆ ")
    st.markdown("- RAG (Retrieval-Augmented Generation)")
    st.markdown("- ParentDocumentRetriever")
    st.markdown("- Ensemble Retriever")

with col2:
    st.markdown("### ğŸŒ± ì§€ì› ì‘ë¬¼")
    st.markdown("- ğŸ“ ë”¸ê¸° ì¬ë°°ë²•")
    st.markdown("- ğŸ¥” ê°ì ì¬ë°°ë²•")
    st.markdown("- ğŸ“š ë†ì—… ê¸°ìˆ  ê°€ì´ë“œ")

with col3:
    st.markdown("### ğŸ’¡ ì‚¬ìš© íŒ")
    st.markdown("- êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”")
    st.markdown("- ë”¸ê¸°/ê°ì í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ì„¸ìš”")
    st.markdown("- ë¹„êµ ì§ˆë¬¸ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤")

# ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
    st.session_state.messages = []
    st.session_state.chat_history = ChatMessageHistory()
    st.rerun() 