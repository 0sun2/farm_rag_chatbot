from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1. ì„ë² ë”© ëª¨ë¸ ë¡œë”©
embedding = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-nli")

# 2. ë²¡í„° DB ë¶ˆëŸ¬ì˜¤ê¸°
retriever = FAISS.load_local("strawberry_potato_FAISS_DB", embedding, allow_dangerous_deserialization=True).as_retriever()

# 3. ë¬¸ì„œ ë¶„ì„ í•¨ìˆ˜ ì¶”ê°€
def analyze_documents(retriever):
    """ë¬¸ì„œì˜ ê¸¸ì´ì™€ ë³µì¡ì„±ì„ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜"""
    print("ğŸ“Š ë¬¸ì„œ ë¶„ì„ ì‹œì‘...")
    
    # FAISSì˜ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
    doc_dict = retriever.vectorstore.docstore._dict
    all_docs = list(doc_dict.values())
    
    doc_lengths = []
    doc_sources = []
    doc_complexity = []
    
    for doc in all_docs:
        content = doc.page_content
        length = len(content)
        doc_lengths.append(length)
        
        # ì¶œì²˜ ì •ë³´ ì¶”ì¶œ
        source = doc.metadata.get('source', 'unknown')
        doc_sources.append(source)
        
        # ë³µì¡ì„± ì§€í‘œ (ë¬¸ì¥ ìˆ˜, ë‹¨ì–´ ìˆ˜ ë“±)
        sentences = content.count('.') + content.count('!') + content.count('?')
        words = len(content.split())
        complexity = words / max(sentences, 1)  # ë¬¸ì¥ë‹¹ í‰ê·  ë‹¨ì–´ ìˆ˜
        doc_complexity.append(complexity)
    
    # ë¶„ì„ ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“ˆ ë¬¸ì„œ í†µê³„:")
    print(f"ì´ ë¬¸ì„œ ìˆ˜: {len(doc_lengths)}")
    print(f"í‰ê·  ê¸¸ì´: {sum(doc_lengths)/len(doc_lengths):.1f}ì")
    print(f"ìµœëŒ€ ê¸¸ì´: {max(doc_lengths)}ì")
    print(f"ìµœì†Œ ê¸¸ì´: {min(doc_lengths)}ì")
    print(f"í‰ê·  ë³µì¡ì„± (ë¬¸ì¥ë‹¹ ë‹¨ì–´ ìˆ˜): {sum(doc_complexity)/len(doc_complexity):.1f}")
    
    # ê¸¸ì´ë³„ ë¶„í¬
    print(f"\nğŸ“ ê¸¸ì´ë³„ ë¶„í¬:")
    length_ranges = {
        "ë§¤ìš° ì§§ìŒ (0-100ì)": len([l for l in doc_lengths if l <= 100]),
        "ì§§ìŒ (101-300ì)": len([l for l in doc_lengths if 101 <= l <= 300]),
        "ë³´í†µ (301-500ì)": len([l for l in doc_lengths if 301 <= l <= 500]),
        "ê¸¸ìŒ (501-800ì)": len([l for l in doc_lengths if 501 <= l <= 800]),
        "ë§¤ìš° ê¸¸ìŒ (800ì+)": len([l for l in doc_lengths if l > 800])
    }
    
    for range_name, count in length_ranges.items():
        percentage = (count / len(doc_lengths)) * 100
        print(f"  {range_name}: {count}ê°œ ({percentage:.1f}%)")
    
    # ì¶œì²˜ë³„ ë¶„ì„
    print(f"\nğŸ“ ì¶œì²˜ë³„ ë¶„ì„:")
    source_counts = {}
    for source in doc_sources:
        source_counts[source] = source_counts.get(source, 0) + 1
    
    for source, count in source_counts.items():
        print(f"  {source}: {count}ê°œ ë¬¸ì„œ")
    
    # ë³µì¡ì„± ë¶„ì„
    print(f"\nğŸ§  ë³µì¡ì„± ë¶„ì„:")
    complexity_ranges = {
        "ë‹¨ìˆœ (ë¬¸ì¥ë‹¹ 10ë‹¨ì–´ ì´í•˜)": len([c for c in doc_complexity if c <= 10]),
        "ë³´í†µ (ë¬¸ì¥ë‹¹ 11-20ë‹¨ì–´)": len([c for c in doc_complexity if 11 <= c <= 20]),
        "ë³µì¡ (ë¬¸ì¥ë‹¹ 21ë‹¨ì–´ ì´ìƒ)": len([c for c in doc_complexity if c > 20])
    }
    
    for complexity_name, count in complexity_ranges.items():
        percentage = (count / len(doc_complexity)) * 100
        print(f"  {complexity_name}: {count}ê°œ ({percentage:.1f}%)")
    
    # RAG ê¸°ë²• ì¶”ì²œ
    print(f"\nğŸ’¡ RAG ê¸°ë²• ì¶”ì²œ:")
    
    avg_length = sum(doc_lengths)/len(doc_lengths)
    avg_complexity = sum(doc_complexity)/len(doc_complexity)
    
    if avg_length > 500:
        print("  âœ… ParentDocumentRetriever ì¶”ì²œ: ë¬¸ì„œê°€ ê¸¸ì–´ì„œ ë¬¸ë§¥ ë³´ì¡´ì´ ì¤‘ìš”")
    elif avg_length < 200:
        print("  âœ… ê¸°ë³¸ Retrieverë¡œ ì¶©ë¶„: ë¬¸ì„œê°€ ì§§ì•„ì„œ ì¶”ê°€ ë¶„í•  ë¶ˆí•„ìš”")
    
    if avg_complexity > 15:
        print("  âœ… Multi-Query Retriever ê³ ë ¤: ë³µì¡í•œ ë¬¸ì¥ êµ¬ì¡°ë¡œ ì¸í•œ ê²€ìƒ‰ ì–´ë ¤ì›€ ê°€ëŠ¥")
    
    if len(set(doc_sources)) > 3:
        print("  âœ… Self-Querying ê³ ë ¤: ë‹¤ì–‘í•œ ì¶œì²˜ì˜ ë©”íƒ€ë°ì´í„° í™œìš© ê°€ëŠ¥")
    
    return {
        'lengths': doc_lengths,
        'sources': doc_sources,
        'complexity': doc_complexity,
        'avg_length': avg_length,
        'avg_complexity': avg_complexity
    }

# 4. ë¬¸ì„œ ë¶„ì„ ì‹¤í–‰
print("ğŸ” ë¬¸ì„œ íŠ¹ì„± ë¶„ì„ ì¤‘...")
analysis_result = analyze_documents(retriever)

# 5. Llama3 ëª¨ë¸ ë¡œë”©
model_id = "naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B"
tokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", load_in_4bit=True,trust_remote_code=True)

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=384,
    temperature=0.7,
    top_p=0.9,
    repetition_penalty=1.1
)

llm = HuggingFacePipeline(pipeline=pipe)

# 6. ì‚¬ìš©ì ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸ ì •ì˜
prompt_template = """
ë‹¹ì‹ ì€ ë†ì—… ë¶„ì•¼ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ëª¨ë¥´ëŠ” ë‚´ìš©ì€ 'ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.'ë¼ê³  ë§í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {question}
ë‹µë³€:
"""
prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

# 7. QA ì²´ì¸ êµ¬ì„±
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True,
    chain_type_kwargs={"prompt": prompt}
)

# 8. ì§ˆë¬¸ ì‹¤í–‰
query = "ë”¸ê¸°ë¥¼ ì‹¬ìœ¼ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ë¼?"
result = qa_chain.invoke(query)

print("\nğŸ’¬ ë‹µë³€:", result["result"])
print("\nğŸ“„ ì°¸ê³  ë¬¸ì„œ:", [doc.metadata.get("source", "ì¶œì²˜ ì—†ìŒ") for doc in result["source_documents"]])

# 9. ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ê¸¸ì´ í™•ì¸
print(f"\nğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ë¶„ì„:")
for i, doc in enumerate(result["source_documents"]):
    doc_length = len(doc.page_content)
    print(f"ë¬¸ì„œ {i+1}: {doc_length}ì - {doc.metadata.get('source', 'ì¶œì²˜ ì—†ìŒ')}")
    
    # ë¬¸ì„œ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
    preview = doc.page_content[:100] + "..." if len(doc.page_content) > 100 else doc.page_content
    print(f"  ë¯¸ë¦¬ë³´ê¸°: {preview}")
    print()
