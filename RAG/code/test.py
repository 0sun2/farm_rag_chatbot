from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain.storage import InMemoryStore
from langchain.retrievers import ParentDocumentRetriever
from langchain.prompts import PromptTemplate
from langchain_huggingface import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain.document_transformers import LongContextReorder
import pickle
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --- 1. LLM ë° ì„ë² ë”© ëª¨ë¸ ë¡œë”© ---
embedding = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-nli")

model_id = "naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B"
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", load_in_4bit=True, trust_remote_code=True)

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,
    temperature=0.7,
    top_p=0.9,
    repetition_penalty=1.1
)
llm = HuggingFacePipeline(pipeline=pipe)

# --- 2. ì €ì¥ëœ DB ë° ë¬¸ì„œ ì €ì¥ì†Œ ë¶ˆëŸ¬ì˜¤ê¸° ---
print("ì €ì¥ëœ DBì™€ ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤...")
vectorstore = FAISS.load_local("advanced_rag_faiss_db", embedding, allow_dangerous_deserialization=True)
with open("advanced_rag_docstore.pkl", "rb") as f:
    store = pickle.load(f)
with open("advanced_rag_parent_docs.pkl", "rb") as f:
    parent_documents = pickle.load(f)
print("ë¡œë”© ì™„ë£Œ.")

# --- 3. ê³ ê¸‰ ë¦¬íŠ¸ë¦¬ë²„ êµ¬ì„± ---
child_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)
pdr = ParentDocumentRetriever(
    vectorstore=vectorstore,
    docstore=store,
    child_splitter=child_splitter,
)
pdr.search_kwargs = {'k': 5}
bm25_retriever = BM25Retriever.from_documents(documents=parent_documents)
bm25_retriever.k = 5
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, pdr],
    weights=[0.6, 0.4]
)

# --- 4. LongContextReorder í›„ì²˜ë¦¬ ì„¤ì • ---
reordering = LongContextReorder()

# --- 5. LCELì„ ì‚¬ìš©í•œ ì²´ì¸ êµ¬ì„± ---
prompt_template = """
ì£¼ì–´ì§„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.

ë‚´ìš©:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:
"""
prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# â—ï¸â—ï¸â—ï¸ ì—ëŸ¬ í•´ê²° ë¶€ë¶„ â—ï¸â—ï¸â—ï¸
# reordering(DocumentTransformer)ì„ ì²´ì¸ì— í†µí•©í•˜ê¸° ìœ„í•´ í•¨ìˆ˜ë¡œ ê°ì‹¸ì¤ë‹ˆë‹¤.
# retrieverê°€ ë¬¸ì„œë¥¼ ë°˜í™˜í•˜ë©´(docs), reordering.transform_documents(docs)ë¡œ ìˆœì„œë¥¼ ë°”ê¾¸ê³ 
# ê·¸ ê²°ê³¼ë¥¼ format_docs í•¨ìˆ˜ë¡œ ë„˜ê²¨ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë§Œë“­ë‹ˆë‹¤.
# RunnableLambdaëŠ” ì´ í•¨ìˆ˜ë¥¼ LCEL ì²´ì¸ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.
reorder_and_format = RunnableLambda(
    lambda docs: format_docs(reordering.transform_documents(docs))
)


# LCEL ì²´ì¸ êµ¬ì„±
# ì´ì œ retrieverì˜ ì¶œë ¥ì„ ë°”ë¡œ reorder_and_format ëŒë‹¤ í•¨ìˆ˜ì— ì—°ê²°í•©ë‹ˆë‹¤.
rag_chain = (
    {"context": ensemble_retriever | reorder_and_format, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# --- 6. ì§ˆë¬¸ ì‹¤í–‰ ---
query = "ë”¸ê¸° ì´‰ì„±ì¬ë°°ë¥¼ í•  ë•Œ ì˜¨ë„ ê´€ë¦¬ëŠ” ì–´ë–»ê²Œ í•´ì•¼ë¼? ìƒìœ¡ ë‹¨ê³„ë³„ë¡œ ìƒì„¸íˆ ì•Œë ¤ì¤˜."
print("\nğŸ’¬ ì§ˆë¬¸:", query)
result = rag_chain.invoke(query)
print("\nâœ… ë‹µë³€:")
print(result)

print("\n\nğŸ“„ ì°¸ê³  ë¬¸ì„œ í™•ì¸:")
retrieved_docs = ensemble_retriever.invoke(query)
for i, doc in enumerate(retrieved_docs):
    print(f"--- ë¬¸ì„œ {i+1} (ì¶œì²˜: {doc.metadata.get('source', 'ì¶œì²˜ ì—†ìŒ')}, í˜ì´ì§€: {doc.metadata.get('page', 'N/A')}) ---")
    print(doc.page_content[:300] + "...")
    print("-" * 50)