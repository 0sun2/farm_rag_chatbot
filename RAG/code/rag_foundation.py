from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# 1. ì„ë² ë”© ëª¨ë¸ ë¡œë”©
embedding = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-nli")

# 2. ë²¡í„° DB ë¶ˆëŸ¬ì˜¤ê¸°
retriever = FAISS.load_local("strawberry_potato_FAISS_DB", embedding, allow_dangerous_deserialization=True).as_retriever()

# 3. Llama3 ëª¨ë¸ ë¡œë”©
model_id = "models--Qwen--Qwen3-8B"
tokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", load_in_4bit=True,trust_remote_code=True)

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=384,
    temperature=0.7,
    top_p=0.9,
    repetition_penalty=1.1
)

llm = HuggingFacePipeline(pipeline=pipe)

# 4. ì‚¬ìš©ì ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸ ì •ì˜
prompt_template = """
ë‹¹ì‹ ì€ ë†ì—… ë¶„ì•¼ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ëª¨ë¥´ëŠ” ë‚´ìš©ì€ 'ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.'ë¼ê³  ë§í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {question}
ë‹µë³€:
"""
prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

# 5. QA ì²´ì¸ êµ¬ì„±
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True,
    chain_type_kwargs={"prompt": prompt}
)

# 6. ì§ˆë¬¸ ì‹¤í–‰
query = "ë”¸ê¸°ë¥¼ ì‹¬ìœ¼ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ë¼?"
result = qa_chain.invoke(query)

print("\nğŸ’¬ ë‹µë³€:", result["result"])
print("\nğŸ“„ ì°¸ê³  ë¬¸ì„œ:", [doc.metadata.get("source", "ì¶œì²˜ ì—†ìŒ") for doc in result["source_documents"]])
