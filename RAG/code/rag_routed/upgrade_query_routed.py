import torch
# --- DeprecationWarning í•´ê²°ì„ ìœ„í•´ import ê²½ë¡œë¥¼ ìµœì‹ ìœ¼ë¡œ ìˆ˜ì •í•©ë‹ˆë‹¤ ---
from langchain.prompts import PromptTemplate, ChatPromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings # ìˆ˜ì •
from langchain_community.retrievers import BM25Retriever # ìˆ˜ì •
from langchain.retrievers import EnsembleRetriever, ParentDocumentRetriever
from langchain.storage import InMemoryStore
from langchain_huggingface import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig
from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableBranch
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_transformers import LongContextReorder # ìˆ˜ì •
import pickle
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.docstore.in_memory import InMemoryDocstore # ìˆ˜ì •

# --- 1. LLM ë° ì„ë² ë”© ëª¨ë¸ ë¡œë”© ---
embedding = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-nli")
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)
model_id = "naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B"
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_id, 
    device_map="auto", 
    quantization_config=quantization_config, # ìµœì‹  ë°©ì‹ ì ìš©
    trust_remote_code=True,
)
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=512)
llm = HuggingFacePipeline(pipeline=pipe)

# --- 2. ì‘ë¬¼ë³„ RAG ì²´ì¸ ìƒì„± í•¨ìˆ˜ ---
def create_rag_chain_for_crop(crop_name_en: str):
    print(f"'{crop_name_en}' ì‘ë¬¼ì˜ DBë¥¼ ë¡œë”©í•˜ì—¬ ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.")
    vectorstore = FAISS.load_local(f"{crop_name_en}_faiss_db", embedding, allow_dangerous_deserialization=True)
    with open(f"{crop_name_en}_docstore.pkl", "rb") as f: store = pickle.load(f)
    with open(f"{crop_name_en}_parent_docs.pkl", "rb") as f: parent_documents = pickle.load(f)

    child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)
    pdr = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)
    bm25_retriever = BM25Retriever.from_documents(documents=parent_documents, k=5)
    ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, pdr], weights=[0.6, 0.4])
    reordering = LongContextReorder()
    prompt = PromptTemplate.from_template("ì£¼ì–´ì§„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.\n\në‚´ìš©:\n{context}\n\nì§ˆë¬¸: {question}\n\në‹µë³€:")

    def format_docs(docs): return "\n\n".join(doc.page_content for doc in docs)
    reorder_and_format = RunnableLambda(lambda docs: format_docs(reordering.transform_documents(docs)))
    rag_chain = ({"context": ensemble_retriever | reorder_and_format, "question": RunnablePassthrough()} | prompt | llm | StrOutputParser())
    return rag_chain

# --- 3. ë¼ìš°í„°(Router) ì²´ì¸ êµ¬ì„± ---
router_template = """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ 'ë”¸ê¸°', 'ê°ì', 'ì¼ë°˜' ì„¸ ê°€ì§€ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ëŠ” ë¼ìš°í„°ì…ë‹ˆë‹¤. ì§ˆë¬¸ì— ì–´ë–¤ ì‘ë¬¼ ì´ë¦„ì´ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ë˜ëŠ”ì§€ í™•ì¸í•˜ê³ , í•´ë‹¹í•˜ëŠ” ì¹´í…Œê³ ë¦¬ ì´ë¦„ë§Œ ë‹µë³€í•˜ì„¸ìš”. ì§ˆë¬¸: {question} ì¹´í…Œê³ ë¦¬:"""
router_prompt = ChatPromptTemplate.from_template(router_template)
router_chain = router_prompt | llm | StrOutputParser()

# --- 4. ê° ì‘ë¬¼ë³„ ì²´ì¸ê³¼ ì¼ë°˜ ì²´ì¸ ë¯¸ë¦¬ ìƒì„± ---
rag_chains = {
    "ë”¸ê¸°": create_rag_chain_for_crop("strawberry"),
    "ê°ì": create_rag_chain_for_crop("potato"),
}
general_chain = RunnableLambda(lambda x: "ì–´ë–¤ ì‘ë¬¼ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì‹œëŠ”ì§€ ëª…í™•íˆ ë§ì”€í•´ì£¼ì„¸ìš”. (ì˜ˆ: ë”¸ê¸°, ê°ì)")

# --- 5. RunnableBranchë¥¼ ì´ìš©í•œ ìµœì¢… ì²´ì¸ ê²°í•© ---
# â—ï¸â—ï¸â—ï¸ ì—ëŸ¬ í•´ê²° ë¶€ë¶„ â—ï¸â—ï¸â—ï¸
# ë¼ìš°í„°ì˜ ì¶œë ¥(topic)ê³¼ ì§ˆë¬¸(question)ì„ ëª¨ë‘ ë°›ì•„ì„œ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
def route(info):
    topic_output = info["topic"].strip().lower() # LLMì˜ ì¶œë ¥ì„ ì†Œë¬¸ìë¡œ ë°”ê¾¸ê³  ê³µë°± ì œê±°
    question = info["question"]
    
    # ì •í™•í•œ ì¼ì¹˜ ëŒ€ì‹ , í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ë¡œ ë¶„ê¸°í•©ë‹ˆë‹¤.
    if "ë”¸ê¸°" in topic_output:
        print(">> 'ë”¸ê¸°' ì²´ì¸ìœ¼ë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.")
        return rag_chains["ë”¸ê¸°"].invoke(question) # í•´ë‹¹ ì²´ì¸ì— ì§ˆë¬¸ì„ ì§ì ‘ ì „ë‹¬í•˜ì—¬ ì‹¤í–‰
    elif "ê°ì" in topic_output:
        print(">> 'ê°ì' ì²´ì¸ìœ¼ë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.")
        return rag_chains["ê°ì"].invoke(question)
    else:
        print(">> 'ì¼ë°˜' ì²´ì¸ìœ¼ë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.")
        return general_chain.invoke(question)

# ì „ì²´ íŒŒì´í”„ë¼ì¸: routerì˜ ê²°ê³¼ì™€ ì›ë³¸ ì§ˆë¬¸ì„ í•¨ê»˜ route í•¨ìˆ˜ë¡œ ì „ë‹¬
full_chain = {"topic": router_chain, "question": lambda x: x["question"]} | RunnableLambda(route)

# --- 6. ì§ˆë¬¸ ì‹¤í–‰ ---
def ask_question(query: str):
    print(f"\nğŸ’¬ ì§ˆë¬¸: {query}")
    result = full_chain.invoke({"question": query})
    print("\nâœ… ë‹µë³€:")
    print(result)
    print("\n" + "="*50)

# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
ask_question("ë”¸ê¸° ì´‰ì„±ì¬ë°° ì‹œ ì˜¨ë„ ê´€ë¦¬ëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”?")
ask_question("ê°ì ì‹œì„¤ì¬ë°°ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í™˜ê²½ìš”ì¸ì€ ë­ì•¼?")
ask_question("ë°°ì¶”ëŠ” ì–´ë–»ê²Œ í‚¤ì›Œ?")